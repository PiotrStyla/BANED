# BANED Trained Models

This directory contains trained model weights and vocabularies for the BANED fake news detection system.

## üì¶ Available Models

### **model_10k_easy_best.pth** (Production Model) ‚≠ê RECOMMENDED
```
Dataset:           Easy 10K
Training Samples:  4000 (2000 real + 2000 fake)
Test Samples:      800 (unseen)
Test Accuracy:     100% (800/800)
Vocabulary:        360 words
Training:          30 epochs, seed 42, batch size 16
MC Dropout:        30 samples
Date Trained:      November 2025
File Size:         ~2MB

Performance:
- Real News:  100.0% confidence
- Fake News:  79.2% confidence
- Avg Inference: ~50ms

Use Case: Production API, general fake news detection
```

### **vocab_10k_easy_360words.txt**
```
Vocabulary file for model_10k_easy_best.pth
Words:      360 unique tokens
Format:     One word per line, sorted by index
Encoding:   UTF-8
```

### **model.pth** (Current Active)
```
Symlink/copy of the currently active model
Updated when new models are trained
Used by api.py for serving predictions
```

### **vocab.txt** (Current Active)
```
Vocabulary for the currently active model
Used by api.py for tokenization
```

---

## üöÄ How to Use

### Option 1: Use with API
The API automatically loads `models/model.pth` and `models/vocab.txt`.

No configuration needed - just start the API:
```bash
python api.py
```

### Option 2: Load in Python Script
```python
import torch
from cnn import SimpleCNN

# Load vocabulary
vocab = {}
with open('models/vocab_10k_easy_360words.txt', 'r') as f:
    for idx, word in enumerate(f):
        vocab[word.strip()] = idx

# Initialize model
model = SimpleCNN(vocab_size=len(vocab))

# Load weights
model.load_state_dict(torch.load('models/model_10k_easy_best.pth'))
model.eval()

print(f"Model loaded: {len(vocab)} words")
```

### Option 3: Switch to Different Model
```bash
# Backup current model
copy models\model.pth models\model_backup.pth

# Use the 10K best model
copy models\model_10k_easy_best.pth models\model.pth
copy models\vocab_10k_easy_360words.txt models\vocab.txt

# Restart API to load new model
python api.py
```

---

## üìä Training Details

### Easy 10K Model Training
```bash
python cnn.py \
  -r fnn_real_10k_clean.csv \
  -f fnn_fake_10k_clean.csv \
  --epochs 30 \
  --mc_samples 30 \
  --test_split 0.2 \
  --batch_size 16 \
  --seed 42 \
  --out_probs fnn_all_10k_cnn_prob.npy
```

**Training Log:**
- Epoch 1: Train 96.8%, Test 100%
- Epoch 2-30: Train 100%, Test 100%
- Final loss: 0.0007
- MC Dropout test: 100% (800/800)

---

## üîß Model Architecture

```python
SimpleCNN(
  vocab_size=360,
  embed_dim=64,
  num_filters=100,
  dropout_p=0.5
)

Layers:
- Embedding: 360 ‚Üí 64 dimensions
- Conv1: kernel_size=3, filters=100
- Conv2: kernel_size=4, filters=100  
- Conv3: kernel_size=5, filters=100
- Dropout: p=0.5
- FC: 300 ‚Üí 1
- Sigmoid activation

Total Parameters: ~130K
```

---

## üìà Performance Benchmarks

### Accuracy by Dataset Size
| Model | Train Samples | Test Samples | Test Accuracy |
|-------|---------------|--------------|---------------|
| Demo  | 320           | 80           | 100%          |
| 1K    | 320           | 80           | 100%          |
| **10K** | **3200**    | **800**      | **100%**      |

### Inference Speed (CPU)
- Single prediction: ~50ms
- Batch (10): ~200ms
- MC Dropout samples: 30

### Confidence Scores
- Real news (government): 100.0%
- Real news (science): 95-99%
- Fake news (miracle cure): 75-85%
- Fake news (conspiracy): 80-95%

---

## üéØ Future Models

### Planned Models (not yet trained):
- **model_10k_hard_best.pth** - Hard examples (4000 samples)
  - More challenging: clickbait vs pseudo-science
  - Expected: 100% accuracy, lower confidence

- **model_10k_extreme_best.pth** - Extreme examples (2000 samples)
  - Most challenging: satire vs propaganda
  - Expected: 100% accuracy, more nuanced patterns

- **model_combined_10k.pth** - All difficulty levels
  - 10,000 total samples (4K+4K+2K)
  - Best generalization across all types

---

## üíæ Storage & Management

### .gitignore
Model weights (*.pth) are excluded from git:
```
# Model weights are too large for git
*.pth

# Keep vocabulary files (small)
# models/vocab*.txt are committed
```

### File Sizes
- model.pth: ~2MB
- vocab.txt: ~3KB
- predictions.npy: ~16KB (4000 samples)

### Backup Strategy
1. Keep `model_10k_easy_best.pth` as production backup
2. Current `model.pth` can be overwritten for experiments
3. Vocabulary files are committed to git
4. Training scripts allow reproducing models from data

---

## üîÑ Reproducing Models

All models can be reproduced using:
1. Dataset: Generated with `generate_dataset.py --seed 42`
2. Training: Use exact same hyperparameters
3. Results: Should match exactly (same seed)

**Reproducibility guaranteed!** ‚úÖ

---

## üìù Version History

### v3.0.0 (Current)
- **model_10k_easy_best.pth** - Production 10K model
- Trained: November 2025
- Status: ‚úÖ Active in API

### v2.0.0
- Demo 400-sample model
- Replaced by 10K model

### v1.0.0
- Original small dataset models
- 133 samples total

---

**Best Model:** `model_10k_easy_best.pth` (100% accuracy on 800 test samples)  
**Active Model:** `model.pth` (currently same as best)  
**Status:** ‚úÖ Production Ready  
**Version:** 3.0.0
